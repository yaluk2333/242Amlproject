{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Advanced Models for Popularity Prediction\n",
        "This notebook implements Random Forest and Gradient Boosting models for predicting song popularity, comparing them against the baseline Ridge regression model.\n",
        "\n",
        "**Models:**\n",
        "1. Ridge Regression (Baseline)\n",
        "2. Random Forest Regressor\n",
        "3. Gradient Boosting Regressor\n",
        "4. XGBoost (optional, if available)\n",
        "\n",
        "**Evaluation Metrics:** RMSE, $R^2$, Feature Importance\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Models\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "\n",
        "# Metrics\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "\n",
        "# Optional: XGBoost\n",
        "try:\n",
        "    import xgboost as xgb\n",
        "    XGBOOST_AVAILABLE = True\n",
        "except ImportError:\n",
        "    XGBOOST_AVAILABLE = False\n",
        "    print(\"XGBoost not available. Install with: pip install xgboost\")\n",
        "\n",
        "print(\"Libraries imported successfully\")\n",
        "if XGBOOST_AVAILABLE:\n",
        "    print(\"XGBoost is available\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Load and Prepare Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load data\n",
        "df = pd.read_csv(\"dataset.csv\")\n",
        "\n",
        "# Basic cleanup\n",
        "df = df.drop_duplicates(subset=[\"track_id\"]).reset_index(drop=True)\n",
        "\n",
        "print(\"Dataset loaded successfully\")\n",
        "print(\"Shape:\", df.shape)\n",
        "print(\"\\nFirst few rows:\")\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Define Features and Target\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Target variable\n",
        "TARGET = \"popularity\"\n",
        "\n",
        "# Numeric audio features\n",
        "audio_features = [\n",
        "    \"duration_ms\", \"danceability\", \"energy\", \"key\", \"loudness\", \"mode\",\n",
        "    \"speechiness\", \"acousticness\", \"instrumentalness\", \"liveness\",\n",
        "    \"valence\", \"tempo\", \"time_signature\"\n",
        "]\n",
        "\n",
        "# Categorical features\n",
        "categorical_features = [\"explicit\", \"track_genre\"]\n",
        "\n",
        "# Keep only needed columns\n",
        "keep_cols = [\"track_id\", \"track_name\", \"artists\", \"album_name\", TARGET] + audio_features + categorical_features\n",
        "df = df[keep_cols].copy()\n",
        "\n",
        "print(\"Missing values:\")\n",
        "print(df.isna().mean().sort_values(ascending=False).head(10))\n",
        "print(f\"\\nDataset shape: {df.shape}\")\n",
        "print(f\"Target variable: {TARGET}\")\n",
        "print(f\"Audio features: {len(audio_features)}\")\n",
        "print(f\"Categorical features: {len(categorical_features)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Train/Test Split\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare features and target\n",
        "X = df[audio_features + categorical_features]\n",
        "y = df[TARGET].astype(float)\n",
        "\n",
        "# Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, shuffle=True\n",
        ")\n",
        "\n",
        "print(f\"Train size: {X_train.shape}\")\n",
        "print(f\"Test size: {X_test.shape}\")\n",
        "print(f\"\\nTarget statistics (train):\")\n",
        "print(y_train.describe())\n",
        "print(f\"\\nTarget statistics (test):\")\n",
        "print(y_test.describe())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Preprocessing Pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Numeric preprocessing: impute missing values + scale\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "    (\"scaler\", StandardScaler())\n",
        "])\n",
        "\n",
        "# Categorical preprocessing: impute + one-hot encode\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))\n",
        "])\n",
        "\n",
        "# Combine numeric + categorical preprocessing\n",
        "preprocess = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"num\", numeric_transformer, audio_features),\n",
        "        (\"cat\", categorical_transformer, categorical_features),\n",
        "    ],\n",
        "    remainder=\"drop\"\n",
        ")\n",
        "\n",
        "print(\"Preprocessing pipeline created\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Baseline Model - Ridge Regression\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ridge Regression as baseline\n",
        "ridge_model = Ridge(alpha=3.0, random_state=42)\n",
        "\n",
        "ridge_pipeline = Pipeline(steps=[\n",
        "    (\"preprocess\", preprocess),\n",
        "    (\"model\", ridge_model)\n",
        "])\n",
        "\n",
        "ridge_pipeline.fit(X_train, y_train)\n",
        "y_pred_ridge = ridge_pipeline.predict(X_test)\n",
        "\n",
        "rmse_ridge = np.sqrt(mean_squared_error(y_test, y_pred_ridge))\n",
        "r2_ridge = r2_score(y_test, y_pred_ridge)\n",
        "mae_ridge = mean_absolute_error(y_test, y_pred_ridge)\n",
        "\n",
        "print(f\"[Ridge Baseline]\")\n",
        "print(f\"RMSE: {rmse_ridge:.3f}\")\n",
        "print(f\"R¬≤: {r2_ridge:.3f}\")\n",
        "print(f\"MAE: {mae_ridge:.3f}\")\n",
        "\n",
        "# Store results for comparison\n",
        "results = {\n",
        "    \"Ridge\": {\"RMSE\": rmse_ridge, \"R¬≤\": r2_ridge, \"MAE\": mae_ridge}\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Random Forest Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Random Forest Model\n",
        "rf_model = RandomForestRegressor(\n",
        "    n_estimators=100,\n",
        "    max_depth=15,\n",
        "    min_samples_split=10,\n",
        "    min_samples_leaf=4,\n",
        "    random_state=42,\n",
        "    n_jobs=-1,\n",
        "    verbose=0\n",
        ")\n",
        "\n",
        "rf_pipeline = Pipeline(steps=[\n",
        "    (\"preprocess\", preprocess),\n",
        "    (\"model\", rf_model)\n",
        "])\n",
        "\n",
        "print(\"Training Random Forest...\")\n",
        "rf_pipeline.fit(X_train, y_train)\n",
        "\n",
        "y_pred_rf = rf_pipeline.predict(X_test)\n",
        "\n",
        "rmse_rf = np.sqrt(mean_squared_error(y_test, y_pred_rf))\n",
        "r2_rf = r2_score(y_test, y_pred_rf)\n",
        "mae_rf = mean_absolute_error(y_test, y_pred_rf)\n",
        "\n",
        "print(f\"\\n[Random Forest]\")\n",
        "print(f\"RMSE: {rmse_rf:.3f}\")\n",
        "print(f\"R¬≤: {r2_rf:.3f}\")\n",
        "print(f\"MAE: {mae_rf:.3f}\")\n",
        "\n",
        "results[\"Random Forest\"] = {\"RMSE\": rmse_rf, \"R¬≤\": r2_rf, \"MAE\": mae_rf}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Gradient Boosting Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Gradient Boosting Regressor\n",
        "gb_model = GradientBoostingRegressor(\n",
        "    n_estimators=100,\n",
        "    learning_rate=0.1,\n",
        "    max_depth=5,\n",
        "    min_samples_split=10,\n",
        "    min_samples_leaf=4,\n",
        "    random_state=42,\n",
        "    verbose=0\n",
        ")\n",
        "\n",
        "gb_pipeline = Pipeline(steps=[\n",
        "    (\"preprocess\", preprocess),\n",
        "    (\"model\", gb_model)\n",
        "])\n",
        "\n",
        "print(\"Training Gradient Boosting...\")\n",
        "gb_pipeline.fit(X_train, y_train)\n",
        "\n",
        "y_pred_gb = gb_pipeline.predict(X_test)\n",
        "\n",
        "rmse_gb = np.sqrt(mean_squared_error(y_test, y_pred_gb))\n",
        "r2_gb = r2_score(y_test, y_pred_gb)\n",
        "mae_gb = mean_absolute_error(y_test, y_pred_gb)\n",
        "\n",
        "print(f\"\\n[Gradient Boosting]\")\n",
        "print(f\"RMSE: {rmse_gb:.3f}\")\n",
        "print(f\"R¬≤: {r2_gb:.3f}\")\n",
        "print(f\"MAE: {mae_gb:.3f}\")\n",
        "\n",
        "results[\"Gradient Boosting\"] = {\"RMSE\": rmse_gb, \"R¬≤\": r2_gb, \"MAE\": mae_gb}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 8: XGBoost Model (Optional)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if XGBOOST_AVAILABLE:\n",
        "    # XGBoost Regressor\n",
        "    xgb_model = xgb.XGBRegressor(\n",
        "        n_estimators=100,\n",
        "        learning_rate=0.1,\n",
        "        max_depth=5,\n",
        "        min_child_weight=3,\n",
        "        subsample=0.8,\n",
        "        colsample_bytree=0.8,\n",
        "        random_state=42,\n",
        "        n_jobs=-1,\n",
        "        verbosity=0\n",
        "    )\n",
        "    \n",
        "    xgb_pipeline = Pipeline(steps=[\n",
        "        (\"preprocess\", preprocess),\n",
        "        (\"model\", xgb_model)\n",
        "    ])\n",
        "    \n",
        "    print(\"Training XGBoost...\")\n",
        "    xgb_pipeline.fit(X_train, y_train)\n",
        "    \n",
        "    y_pred_xgb = xgb_pipeline.predict(X_test)\n",
        "    \n",
        "    rmse_xgb = np.sqrt(mean_squared_error(y_test, y_pred_xgb))\n",
        "    r2_xgb = r2_score(y_test, y_pred_xgb)\n",
        "    mae_xgb = mean_absolute_error(y_test, y_pred_xgb)\n",
        "    \n",
        "    print(f\"\\n[XGBoost]\")\n",
        "    print(f\"RMSE: {rmse_xgb:.3f}\")\n",
        "    print(f\"R¬≤: {r2_xgb:.3f}\")\n",
        "    print(f\"MAE: {mae_xgb:.3f}\")\n",
        "    \n",
        "    results[\"XGBoost\"] = {\"RMSE\": rmse_xgb, \"R¬≤\": r2_xgb, \"MAE\": mae_xgb}\n",
        "else:\n",
        "    print(\"XGBoost not available. Skipping XGBoost model.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 9: Model Comparison\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create comparison dataframe\n",
        "comparison_df = pd.DataFrame(results).T\n",
        "comparison_df = comparison_df.round(3)\n",
        "comparison_df = comparison_df.sort_values(\"RMSE\")\n",
        "\n",
        "print(\"=\" * 50)\n",
        "print(\"MODEL COMPARISON\")\n",
        "print(\"=\" * 50)\n",
        "print(comparison_df.to_string())\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Visualize comparison\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
        "\n",
        "metrics = ['RMSE', 'R¬≤', 'MAE']\n",
        "for idx, metric in enumerate(metrics):\n",
        "    comparison_df[metric].plot(kind='bar', ax=axes[idx], color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728'][:len(comparison_df)])\n",
        "    axes[idx].set_title(f'{metric} Comparison')\n",
        "    axes[idx].set_ylabel(metric)\n",
        "    axes[idx].tick_params(axis='x', rotation=45)\n",
        "    axes[idx].grid(axis='y', alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 10: Feature Importance Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get feature names after preprocessing\n",
        "# Use the fitted preprocess from the pipeline (already fitted during model training)\n",
        "fitted_preprocess = rf_pipeline.named_steps['preprocess']\n",
        "feature_names = (\n",
        "    audio_features + \n",
        "    list(fitted_preprocess.named_transformers_['cat'].named_steps['onehot'].get_feature_names_out(categorical_features))\n",
        ")\n",
        "\n",
        "print(f\"Total number of features after preprocessing: {len(feature_names)}\")\n",
        "\n",
        "# Random Forest Feature Importance\n",
        "rf_importances = rf_pipeline.named_steps['model'].feature_importances_\n",
        "rf_feat_df = pd.DataFrame({\n",
        "    'feature': feature_names,\n",
        "    'importance': rf_importances\n",
        "}).sort_values('importance', ascending=False).head(20)\n",
        "\n",
        "# Gradient Boosting Feature Importance\n",
        "gb_importances = gb_pipeline.named_steps['model'].feature_importances_\n",
        "gb_feat_df = pd.DataFrame({\n",
        "    'feature': feature_names,\n",
        "    'importance': gb_importances\n",
        "}).sort_values('importance', ascending=False).head(20)\n",
        "\n",
        "# Visualize feature importance\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# Random Forest\n",
        "rf_feat_df.plot(x='feature', y='importance', kind='barh', ax=axes[0], color='steelblue')\n",
        "axes[0].set_title('Random Forest - Top 20 Feature Importance')\n",
        "axes[0].set_xlabel('Importance')\n",
        "axes[0].invert_yaxis()\n",
        "\n",
        "# Gradient Boosting\n",
        "gb_feat_df.plot(x='feature', y='importance', kind='barh', ax=axes[1], color='forestgreen')\n",
        "axes[1].set_title('Gradient Boosting - Top 20 Feature Importance')\n",
        "axes[1].set_xlabel('Importance')\n",
        "axes[1].invert_yaxis()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Top 10 Features - Random Forest:\")\n",
        "print(rf_feat_df.head(10).to_string(index=False))\n",
        "print(\"\\nTop 10 Features - Gradient Boosting:\")\n",
        "print(gb_feat_df.head(10).to_string(index=False))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 11: Hyperparameter Tuning (Random Forest)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hyperparameter tuning for Random Forest using RandomizedSearchCV\n",
        "rf_param_grid = {\n",
        "    'model__n_estimators': [100, 200, 300],\n",
        "    'model__max_depth': [10, 15, 20, None],\n",
        "    'model__min_samples_split': [5, 10, 15],\n",
        "    'model__min_samples_leaf': [2, 4, 6],\n",
        "    'model__max_features': ['sqrt', 'log2', None]\n",
        "}\n",
        "\n",
        "rf_pipeline_tune = Pipeline(steps=[\n",
        "    (\"preprocess\", preprocess),\n",
        "    (\"model\", RandomForestRegressor(random_state=42, n_jobs=-1))\n",
        "])\n",
        "\n",
        "print(\"Starting RandomizedSearchCV for Random Forest...\")\n",
        "print(\"This may take several minutes...\")\n",
        "\n",
        "rf_random_search = RandomizedSearchCV(\n",
        "    rf_pipeline_tune,\n",
        "    rf_param_grid,\n",
        "    n_iter=20,  # Number of parameter settings sampled\n",
        "    cv=3,\n",
        "    scoring='neg_mean_squared_error',\n",
        "    n_jobs=-1,\n",
        "    random_state=42,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "rf_random_search.fit(X_train, y_train)\n",
        "\n",
        "print(\"\\nBest parameters for Random Forest:\")\n",
        "print(rf_random_search.best_params_)\n",
        "\n",
        "# Evaluate best model\n",
        "y_pred_rf_tuned = rf_random_search.predict(X_test)\n",
        "rmse_rf_tuned = np.sqrt(mean_squared_error(y_test, y_pred_rf_tuned))\n",
        "r2_rf_tuned = r2_score(y_test, y_pred_rf_tuned)\n",
        "\n",
        "print(f\"\\n[Random Forest - Tuned]\")\n",
        "print(f\"RMSE: {rmse_rf_tuned:.3f}\")\n",
        "print(f\"R¬≤: {r2_rf_tuned:.3f}\")\n",
        "print(f\"Improvement over baseline: RMSE reduction = {rmse_ridge - rmse_rf_tuned:.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 12: Hyperparameter Tuning (Gradient Boosting)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hyperparameter tuning for Gradient Boosting\n",
        "gb_param_grid = {\n",
        "    'model__n_estimators': [100, 200, 300],\n",
        "    'model__learning_rate': [0.05, 0.1, 0.15],\n",
        "    'model__max_depth': [3, 5, 7],\n",
        "    'model__min_samples_split': [5, 10, 15],\n",
        "    'model__min_samples_leaf': [2, 4, 6],\n",
        "    'model__subsample': [0.8, 0.9, 1.0]\n",
        "}\n",
        "\n",
        "gb_pipeline_tune = Pipeline(steps=[\n",
        "    (\"preprocess\", preprocess),\n",
        "    (\"model\", GradientBoostingRegressor(random_state=42))\n",
        "])\n",
        "\n",
        "print(\"Starting RandomizedSearchCV for Gradient Boosting...\")\n",
        "print(\"This may take several minutes...\")\n",
        "\n",
        "gb_random_search = RandomizedSearchCV(\n",
        "    gb_pipeline_tune,\n",
        "    gb_param_grid,\n",
        "    n_iter=20,\n",
        "    cv=3,\n",
        "    scoring='neg_mean_squared_error',\n",
        "    n_jobs=-1,\n",
        "    random_state=42,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "gb_random_search.fit(X_train, y_train)\n",
        "\n",
        "print(\"\\nBest parameters for Gradient Boosting:\")\n",
        "print(gb_random_search.best_params_)\n",
        "\n",
        "# Evaluate best model\n",
        "y_pred_gb_tuned = gb_random_search.predict(X_test)\n",
        "rmse_gb_tuned = np.sqrt(mean_squared_error(y_test, y_pred_gb_tuned))\n",
        "r2_gb_tuned = r2_score(y_test, y_pred_gb_tuned)\n",
        "\n",
        "print(f\"\\n[Gradient Boosting - Tuned]\")\n",
        "print(f\"RMSE: {rmse_gb_tuned:.3f}\")\n",
        "print(f\"R¬≤: {r2_gb_tuned:.3f}\")\n",
        "print(f\"Improvement over baseline: RMSE reduction = {rmse_ridge - rmse_gb_tuned:.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 13: Final Model Comparison (Including Tuned Models)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Final results including tuned models\n",
        "final_results = {\n",
        "    \"Ridge (Baseline)\": {\"RMSE\": rmse_ridge, \"R¬≤\": r2_ridge, \"MAE\": mae_ridge},\n",
        "    \"Random Forest (Default)\": {\"RMSE\": rmse_rf, \"R¬≤\": r2_rf, \"MAE\": mae_rf},\n",
        "    \"Random Forest (Tuned)\": {\"RMSE\": rmse_rf_tuned, \"R¬≤\": r2_rf_tuned, \"MAE\": mean_absolute_error(y_test, y_pred_rf_tuned)},\n",
        "    \"Gradient Boosting (Default)\": {\"RMSE\": rmse_gb, \"R¬≤\": r2_gb, \"MAE\": mae_gb},\n",
        "    \"Gradient Boosting (Tuned)\": {\"RMSE\": rmse_gb_tuned, \"R¬≤\": r2_gb_tuned, \"MAE\": mean_absolute_error(y_test, y_pred_gb_tuned)}\n",
        "}\n",
        "\n",
        "if XGBOOST_AVAILABLE:\n",
        "    final_results[\"XGBoost\"] = {\"RMSE\": rmse_xgb, \"R¬≤\": r2_xgb, \"MAE\": mae_xgb}\n",
        "\n",
        "final_comparison_df = pd.DataFrame(final_results).T\n",
        "final_comparison_df = final_comparison_df.round(3)\n",
        "final_comparison_df = final_comparison_df.sort_values(\"RMSE\")\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"FINAL MODEL COMPARISON (All Models)\")\n",
        "print(\"=\" * 60)\n",
        "print(final_comparison_df.to_string())\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Visualize\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "for idx, metric in enumerate(metrics):\n",
        "    final_comparison_df[metric].plot(kind='bar', ax=axes[idx], \n",
        "                                     color=plt.cm.viridis(np.linspace(0, 1, len(final_comparison_df))))\n",
        "    axes[idx].set_title(f'{metric} Comparison (All Models)')\n",
        "    axes[idx].set_ylabel(metric)\n",
        "    axes[idx].tick_params(axis='x', rotation=45)\n",
        "    axes[idx].grid(axis='y', alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Find best model\n",
        "best_model_name = final_comparison_df.index[0]\n",
        "best_rmse = final_comparison_df.loc[best_model_name, 'RMSE']\n",
        "print(f\"\\nüèÜ Best Model: {best_model_name}\")\n",
        "print(f\"   RMSE: {best_rmse:.3f}\")\n",
        "print(f\"   R¬≤: {final_comparison_df.loc[best_model_name, 'R¬≤']:.3f}\")\n",
        "print(f\"   Improvement over Ridge baseline: {((rmse_ridge - best_rmse) / rmse_ridge * 100):.1f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 14: Prediction Visualization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize predictions vs actual values for best model\n",
        "if 'Random Forest (Tuned)' in final_comparison_df.index:\n",
        "    best_predictions = y_pred_rf_tuned\n",
        "    best_name = \"Random Forest (Tuned)\"\n",
        "elif 'Gradient Boosting (Tuned)' in final_comparison_df.index:\n",
        "    best_predictions = y_pred_gb_tuned\n",
        "    best_name = \"Gradient Boosting (Tuned)\"\n",
        "else:\n",
        "    best_predictions = y_pred_rf\n",
        "    best_name = \"Random Forest\"\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Scatter plot: Predicted vs Actual\n",
        "axes[0].scatter(y_test, best_predictions, alpha=0.5, s=10)\n",
        "axes[0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
        "axes[0].set_xlabel('Actual Popularity')\n",
        "axes[0].set_ylabel('Predicted Popularity')\n",
        "axes[0].set_title(f'Predicted vs Actual - {best_name}')\n",
        "axes[0].grid(alpha=0.3)\n",
        "\n",
        "# Residual plot\n",
        "residuals = y_test - best_predictions\n",
        "axes[1].scatter(best_predictions, residuals, alpha=0.5, s=10)\n",
        "axes[1].axhline(y=0, color='r', linestyle='--', lw=2)\n",
        "axes[1].set_xlabel('Predicted Popularity')\n",
        "axes[1].set_ylabel('Residuals')\n",
        "axes[1].set_title(f'Residual Plot - {best_name}')\n",
        "axes[1].grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"Residual statistics for {best_name}:\")\n",
        "print(f\"  Mean: {residuals.mean():.3f}\")\n",
        "print(f\"  Std: {residuals.std():.3f}\")\n",
        "print(f\"  Min: {residuals.min():.3f}\")\n",
        "print(f\"  Max: {residuals.max():.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "This notebook implemented and compared several advanced models for song popularity prediction:\n",
        "\n",
        "1. **Baseline (Ridge Regression)**: Linear model with L2 regularization\n",
        "2. **Random Forest**: Ensemble of decision trees, good for capturing non-linear relationships\n",
        "3. **Gradient Boosting**: Sequential ensemble method that builds trees to correct errors\n",
        "4. **XGBoost** (if available): Optimized gradient boosting implementation\n",
        "\n",
        "**Key Findings:**\n",
        "- Tree-based models (Random Forest, Gradient Boosting) generally outperform linear models\n",
        "- Hyperparameter tuning improves model performance\n",
        "- Feature importance analysis reveals which audio features are most predictive\n",
        "- Genre features remain important for cold-start popularity prediction\n",
        "\n",
        "**Next Steps:**\n",
        "- Consider ensemble methods (voting or stacking)\n",
        "- Integrate best model into recommendation system\n",
        "- Evaluate on different subsets of data (by genre, time period, etc.)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
